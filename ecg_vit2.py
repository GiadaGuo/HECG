import argparse
import os
import sys
import datetime
import time
import math
import json
from pathlib import Path

import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.distributed as dist
import torch.backends.cudnn as cudnn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torchvision import models as torchvision_models

import utils
import vision_transformer as vits
from vision_transformer import DINOHead

import math
from functools import partial

import torch
import torch.nn as nn

#from utils import trunc_normal_



def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)



def drop_path(x, drop_prob: float = 0., training: bool = False):
    #如果 drop_prob 为 0 或者当前不是训练模式（training=False），则直接返回输入张量 x
    if drop_prob == 0. or not training:
        return x
    #保留路径的概率
    keep_prob = 1 - drop_prob
    #生成随机掩码：在批量维度上随机丢弃路径，而对其他维度保持一致
    #元组运算：(x.shape[0],)+（1,...,1）=(x.shape[0],1,...,1)
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    #随机张量的每个元素的值将在 [keep_prob, 1 + keep_prob) 之间。
    #torch.rand：从均匀分布 [0,1) 中随机采样指定shape张量
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    #binarize：将随机张量中的每个元素向下取整，将其二值化为 0 或 1
    #random_tensor现在是元素为0，1的张量：每个元素以 keep_prob 的概率为 1，以 drop_prob 的概率为 0
    random_tensor.floor_()

    #除以keep_prob目的：保持输出的期望值（均值）不变
    #E(yi)=keep_prob*(xi/keep_prob)+drop_prob*0=xi
    #.div():对张量中的每个元素进行逐元素除法操作
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """
    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    #qkv_bias: 是否在 Q、K、V 的线性变换中使用偏置，默认值为 False
    #attn_drop: 注意力权重的 dropout 比率，默认值为 0
    #proj_drop: 线性投影层的 dropout 比率，默认值为 0
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads #多头注意力机制中的头数，默认值8
        head_dim = dim // num_heads #每个头的特征维度
        #缩放因子，用于缩放 Q 和 K 的点积。
        #如果 qk_scale 为 None，则使用默认值 head_dim ** -0.5（常见公式里除的\sqrt(d_k)）
        self.scale = qk_scale or head_dim ** -0.5

        #将输入特征映射到 Q、K、V 三个向量：用一个权重矩阵把输入向量一次性投影成三份不同的向量（Q、K、V）
        #默认只作用于输入张量的最后一个维度（默认batch-first，前面的都当成是“多少个样本”或“序列结构”）
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape #(1,257,192)
        #self.qkv(x)->(1,257,192*3)=(1,257,576)
        #.reshape(B, N, 3, self.num_heads, C // self.num_heads)->(1,257,3,8,24)
        #.permute(2, 0, 3, 1, 4)->(3,1,8,257,24)
        #可以把24理解成每个head下q,k,v的长度
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        #拆分第一维度，q,k,v维度均值 (1,1,8,257,24)
        q, k, v = qkv[0], qkv[1], qkv[2]

        #q @ k.transpose(-2, -1):(1,1,8,257,24)*(1,1,8,24,257)->(1,1,8,257,257)
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        #attn @ v：(1,1,8,257,257)*(1,1,8,257,24)->(1,1,8,257,24)
        #.transpose(1, 2):(1,8,1,257,24)
        #.reshape(B, N, C):(1,8,1,257,24)->(1,257,192)  reshape重排成指定维度——concat all the attention heads
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x) #1个linear去实现合并后各head的相对重要性
        x = self.proj_drop(x)
        # x:attention分数——(1,257,192)
        # attn:注意力权重矩阵(表示每个查询向量对每个键向量的注意力权重)
        return x, attn

# 使用参数：Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,
# qkv_bias=qkv_bias, qk_scale=qk_scale,drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x, return_attention=False):
        y, attn = self.attn(self.norm1(x)) #y是attention分数(1,257,192)，attn是注意力权重矩阵
        if return_attention:
            return attn
        x = x + self.drop_path(y)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x #(1,257,192)


class VisionTransformer4K(nn.Module):
    """ Vision Transformer 4K """
    def __init__(self, num_classes=0, img_size=[224], input_embed_dim=384, output_embed_dim = 192,
                 depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, 
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, num_prototypes=64, **kwargs):
        super().__init__()
        embed_dim = output_embed_dim
        self.num_features = self.embed_dim = embed_dim
        self.phi = nn.Sequential(*[nn.Linear(input_embed_dim, output_embed_dim), nn.GELU(), nn.Dropout(p=drop_rate)])

        num_patches = int(img_size[0] // 16)**2
        print("# of Patches:", num_patches)
        
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks = nn.ModuleList([ #存储 nn.Module 的子模块,类似列表
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
            for i in range(depth)])
        self.norm = norm_layer(embed_dim)

        # Classifier head
        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        trunc_normal_(self.pos_embed, std=.02)
        trunc_normal_(self.cls_token, std=.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def interpolate_pos_encoding(self, x, w, h):
        npatch = x.shape[1] - 1
        N = self.pos_embed.shape[1] - 1
        if npatch == N and w == h:
            return self.pos_embed
        class_pos_embed = self.pos_embed[:, 0]
        patch_pos_embed = self.pos_embed[:, 1:]
        dim = x.shape[-1]
        w0 = w // 1
        h0 = h // 1
        # we add a small number to avoid floating point error in the interpolation
        # see discussion at https://github.com/facebookresearch/dino/issues/8
        w0, h0 = w0 + 0.1, h0 + 0.1
        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),
            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),
            mode='bicubic',
        )
        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]
        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)

    def prepare_tokens(self, x):
        #print('preparing tokens (after crop)', x.shape)
        self.mpp_feature = x
        B, embed_dim, w, h = x.shape #e.g.(1,384,16,16)
        #flatten(2, 3) :将从第 2 维到第 3 维的维度进行展平(w 和 h 两个维度合并成一个维度)
        x = x.flatten(2, 3).transpose(1,2)  #e.g.(1,384,256)=> (1,256,384)

        x = self.phi(x) #(1, 256, 192)


        # add the [CLS] token to the embed patch tokens
        cls_tokens = self.cls_token.expand(B, -1, -1) #(1, 1, 192)
        x = torch.cat((cls_tokens, x), dim=1) #(1, 257, 192)

        # add positional encoding to each token
        x = x + self.interpolate_pos_encoding(x, w, h)

        return self.pos_drop(x) #输出维度(1, 257, 192)

    def forward(self, x):
        x = self.prepare_tokens(x)
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x) #(1,257,192)
        #选择了第0维度的所有；第1维的第一个元素；第2维（特征维度）的大小 192 保持不变
        return x[:, 0] #(1,192)

    def get_last_selfattention(self, x):
        x = self.prepare_tokens(x)
        for i, blk in enumerate(self.blocks):
            if i < len(self.blocks) - 1:
                x = blk(x)
            else:
                # return attention of the last block
                return blk(x, return_attention=True)

    def get_intermediate_layers(self, x, n=1):
        x = self.prepare_tokens(x)
        # we return the output tokens from the `n` last blocks
        output = []
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if len(self.blocks) - i <= n:
                output.append(self.norm(x))
        return output
    
def vit4k_xs(patch_size=16, **kwargs):
    model = VisionTransformer4K(
        patch_size=patch_size, input_embed_dim=384, output_embed_dim=192,
        depth=6, num_heads=6, mlp_ratio=4, 
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
